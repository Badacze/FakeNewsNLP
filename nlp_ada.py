# -*- coding: utf-8 -*-
"""NLP_ADA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xMOcjzJxHzWrEd47X29Nh-jVcPvgXyea

# Zbiór treningowy używany jest do stworzenia grafu a testowy do zapytań
# Przy testowaniu metod podobienstwa zmienic je mozna w grafie i metodzie do predykcji

#Importy
"""

import pandas as pd
import numpy as np
import openai
from tenacity import retry, wait_random_exponential, stop_after_attempt
from string import punctuation
import re


import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer


from sentence_transformers import SentenceTransformer

import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity


from sklearn.metrics import classification_report

"""#Ładowanie danych i pozostawienie tylko contentu i target(true/false)"""

train_df = pd.read_csv('train_filtered.tsv',sep='\t')
test_df = pd.read_csv('/content/test_filtered.tsv',sep='\t' )

train_df = train_df.iloc[:,[1,2]]
test_df = test_df.iloc[:,[1,2]]

train_df.columns = ['target','content']
test_df.columns = ['target','content']

for i in [1,10,100,1000]:
  print(train_df['content'][i])

train_df.info()

train_df.head()

"""# Czyszczenie tekstu"""

def clean_text(text):
    temp = text.lower()
    temp = re.sub('\d', '', temp)
    temp = re.sub('<[^>]*>', '', temp)
    emojis = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', temp)
    temp = re.sub('[\W]+', ' ', temp) + ' '.join(emojis).replace('-', '')
    temp = re.sub('[{}]'.format(punctuation), '', temp)
    temp = temp.strip()
    return temp

train_df['content']=train_df['content'].apply(clean_text)
test_df['content']=test_df['content'].apply(clean_text)


for i in [1,10,100,1000]:
  print(train_df['content'][i])

"""# Dalszy preprocesing"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import re

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('punkt_tab')

# text->token->-stopowrds->lemantyzacja/stemming(raczej użyc jednego)->text
def preprocesing(text):
  tokens = nltk.word_tokenize(text)

  stop_words = set(stopwords.words('english'))
  tokens = [word for word in tokens if word not in stop_words]

  #lemmatizer = WordNetLemmatizer()
  #tokens = [lemmatizer.lemmatize(word) for word in tokens]

  stemmer = PorterStemmer()
  tokens = [stemmer.stem(word) for word in tokens]

  temp = ' '.join(tokens)

  # Usuwanie dodatkowych spacji
  temp = re.sub(r'\s+', ' ', temp).strip()

  return temp

train_df['content']=train_df['content'].apply(preprocesing)
test_df['content']=test_df['content'].apply(preprocesing)

for i in [1,10,100,1000]:
  print(train_df['content'][i])

"""# Embeding"""

#uproszczony BERT oraz w wersji dla zdan SBERT - embeding liczony dla zdania
model = SentenceTransformer('all-MiniLM-L6-v2')

train_embeddings = model.encode(train_df['content'].tolist(), show_progress_bar=True)
test_embeddings = model.encode(test_df['content'].tolist(), show_progress_bar=True)

print(train_embeddings)
print(test_embeddings)

"""# Budowa grafu"""

G = nx.Graph()

#wierzchołki
for i in range(len(train_df)):
    G.add_node(i, label=train_df.iloc[i]['target'])

# Cosine similarity + threshold
similarities = cosine_similarity(train_embeddings)
threshold = 0.7

for i in range(len(train_df)):
    for j in range(i+1, len(train_df)):
        if similarities[i][j] >= threshold:
            G.add_edge(i, j, weight=similarities[i][j])

"""# Predykcja"""

#przyjmuje embeding, szuka wszystkich sasiadow sasiadow i na podstawie wiekszosci wyznacza t/f

def predict(emb, G, train_embeddings):
    # 1. Znajdź najbardziej podobny wierzchołek
    sims = cosine_similarity([emb], train_embeddings)[0]
    top1_idx = sims.argmax()

    # 2. Weź jego sąsiadów w grafie
    neighbors = list(G.neighbors(top1_idx))



    voting_nodes = [top1_idx] + neighbors

    voting_labels = [G.nodes[n]['label'] for n in voting_nodes]


    true_count = 0

    for i in voting_labels:
      if i == True:
        true_count += 1


    if true_count > len(voting_labels) / 2:
        return 1
    else:
        return 0


print(predict(test_embeddings[1], G, train_embeddings))

"""#Ewaluacja"""

from sklearn.metrics import classification_report

y_true = [1 if i  == True else 0 for i in test_df['target'].tolist()]
y_pred = [predict(emb, G, train_embeddings) for emb in test_embeddings]

print(y_pred)

print(classification_report(y_true, y_pred, target_names=['FAKE', 'TRUE']))

print(pd.Series(y_true).value_counts())